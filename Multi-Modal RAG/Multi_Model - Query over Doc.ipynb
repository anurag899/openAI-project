{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d4f79f",
   "metadata": {},
   "source": [
    "Need a short video of Bugs Bunny the cartoon character saying “That’s All Folks”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe44fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c347992",
   "metadata": {},
   "source": [
    "- Load Vector Database\n",
    "- Run Multi-Model Chain\n",
    "- Display Anser and Relevant Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa020c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz \n",
    "import io\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "import json\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "from PIL import Image,ImageFile\n",
    "from DocumentLoader import DocumentChunk,DocumentExtract\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "import uuid\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00400c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba410c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5318ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Make sure to be concise and to the point in your answer\\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = AzureChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024,openai_api_version=\"2023-07-01-preview\",\n",
    "    azure_deployment=\"imageanalysis\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa48ed",
   "metadata": {},
   "source": [
    "## Load Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7198a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = ''\n",
    "vectorstore = Chroma(collection_name=collection_name,persist_directory=\"./chroma_db\", embedding_function=embed_model)\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "with open(f'chroma_db/{collection_name}.json', \"r\") as json_file:\n",
    "    loaded_data = json.load(json_file)\n",
    "\n",
    "retriever.docstore.store = loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3da1a6",
   "metadata": {},
   "source": [
    "## Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cde8d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eea8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multi_model = multi_modal_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4da1a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26728fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multi_model.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18a4a266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14569b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
