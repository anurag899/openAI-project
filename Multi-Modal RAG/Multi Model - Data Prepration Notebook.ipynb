{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c714266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d29034",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c55a4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz \n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "from PIL import Image,ImageFile\n",
    "from DocumentLoader import DocumentChunk,DocumentExtract\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889be34",
   "metadata": {},
   "source": [
    "## Class to Extract Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac280b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionImage:\n",
    "    def __init__(self,filename):\n",
    "        self.output_dir = filename.split('.',1)[0]\n",
    "        self.output_format = 'png'\n",
    "        self.min_width = 100\n",
    "        self.min_height = 100\n",
    "        self.filename = filename\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            \n",
    "    def _open_pdf(self):\n",
    "        pdf_file = fitz.open(self.filename)\n",
    "        return pdf_file\n",
    "    \n",
    "    def generate_images(self):\n",
    "        pdf_file = self._open_pdf()\n",
    "        for page_index in range(len(pdf_file)):\n",
    "            # Get the page itself\n",
    "            page = pdf_file[page_index]\n",
    "            # Get image list\n",
    "            image_list = page.get_images(full=True)\n",
    "            # Print the number of images found on this page\n",
    "            if image_list:\n",
    "                print(f\"[+] Found a total of {len(image_list)} images in page {page_index}\")\n",
    "            else:\n",
    "                print(f\"[!] No images found on page {page_index}\")\n",
    "            # Iterate over the images on the page\n",
    "            for image_index, img in enumerate(image_list, start=1):\n",
    "                # Get the XREF of the image\n",
    "                xref = img[0]\n",
    "                # Extract the image bytes\n",
    "                base_image = pdf_file.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                # Get the image extension\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                # Load it to PIL\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                # Check if the image meets the minimum dimensions and save it\n",
    "                if image.width >= self.min_width and image.height >= self.min_height:\n",
    "                    image.save(\n",
    "                        open(os.path.join(self.output_dir, f\"image{page_index + 1}_{image_index}.{self.output_format}\"), \"wb\"),\n",
    "                        format=self.output_format.upper())\n",
    "                else:\n",
    "                    print(f\"[-] Skipping image {image_index} on page {page_index} due to its small size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53598e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4V_keys = \"<gpt4-v api keys>\"\n",
    "GPT4V_ENDPOINT = \"<gpt4-v api endpoint>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015617f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"<azure form recognizer endpoint>\"\n",
    "keys = \"<azure form recognizer keys>\"\n",
    "model_id = \"prebuilt-layout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36a6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '<fileName>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734a5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "docChunk = DocumentChunk.load_keys(endpoint,keys,model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f21cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 300\n",
    "data = docChunk.get_dataframe(filename,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2144b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_table'] = data['content'].str.contains('<table-start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a00fe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  7,  8,  9, 10, 11, 13, 14, 16, 17, 18, 22, 23, 24, 25, 26, 27,\n",
       "       28, 29, 30, 31, 32, 33, 34, 39, 40], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['is_table']==True].page_number.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325821e9",
   "metadata": {},
   "source": [
    "## Generating Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0deef88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVisionCall:\n",
    "    def __init__(self,GPT4V_KEY,GPT4V_ENDPOINT):\n",
    "        self.endpoint = GPT4V_ENDPOINT\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": GPT4V_KEY,\n",
    "        }\n",
    "        \n",
    "    def _get_encoded_image(self,image):\n",
    "        encoded_image = base64.b64encode(open(image, 'rb').read()).decode('ascii')\n",
    "        return encoded_image\n",
    "    \n",
    "    def _call_api(self,messages):\n",
    "        # Payload for the request\n",
    "        payload = {\n",
    "           \"enhancements\": {\n",
    "            \"ocr\": {\n",
    "              \"enabled\": True\n",
    "            },\n",
    "            \"grounding\": {\n",
    "              \"enabled\": True\n",
    "            }},\n",
    "            \"dataSources\": [\n",
    "            {\n",
    "                \"type\": \"AzureComputerVision\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": \"https://imageanalysisvision.cognitiveservices.azure.com/\",\n",
    "                    \"key\": \"b60781f6d99c4869886792006560b097\"\n",
    "                }\n",
    "            }],\n",
    "          \"messages\":messages,\n",
    "          \"temperature\": 0,\n",
    "           \"max_tokens\": 4096,\n",
    "        }\n",
    "        # Send request\n",
    "        try:\n",
    "            response = requests.post(GPT4V_ENDPOINT, headers=self.headers, json=payload)\n",
    "            response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf560396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path):\n",
    "    gptv_obj = GPTVisionCall(GPT4V_keys,GPT4V_ENDPOINT)\n",
    "    encoded_image = gptv_obj._get_encoded_image(image_path)\n",
    "    #sys_prompt = 'Your tasks is to generate detailed summary of the chart'\n",
    "    sys_prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw image. \\\n",
    "        Give a detailed summary of the image that is well optimized for retrieval.\"\"\"\n",
    "    user_txt_prompt = \"\"\"Let's think step by step, and Provide detailed summary for the image. Make sure to follow all instructions\"\"\"\n",
    "    user_img_prompt = f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "    messages = [{'role':'system','content':sys_prompt},\n",
    "           {'role':'user','content':[{\"type\": \"text\",\"text\": user_txt_prompt},\n",
    "            {\"type\": \"image_url\",\"image_url\": {\"url\": user_img_prompt}}]}]\n",
    "    res_json = gptv_obj._call_api(messages)\n",
    "    return (encoded_image,res_json['choices'][0]['message']['content']) if res_json else (encoded_image,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1841884",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"Path to image directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7aff3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images_summaries = [] \n",
    "img_base64_list = []\n",
    "dataframe = pd.DataFrame(columns=['image_name','summary','encoding'])\n",
    "for path in os.listdir(image_path):\n",
    "    img_path = os.path.join(image_path,path)\n",
    "    encoded_img,summary = process_single_image(img_path)\n",
    "    img_base64_list.append(encoded_img)\n",
    "    images_summaries.append(summary)\n",
    "    dataframe.loc[len(dataframe)] = [path,summary,encoded_img]\n",
    "    import time\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "085e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    " \n",
    "client = AzureOpenAI(\n",
    "  api_key = \"<api-keys>\",  \n",
    "  api_version = \"2023-05-15\",\n",
    "  azure_endpoint = \"<azure endpoint>\"\n",
    ")\n",
    " \n",
    "\n",
    "def _gpt_text_summary(client,sys_prompt,user_prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "      model='gpt-4',\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8b7b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_text_chunk(element):\n",
    "    sys_prompt = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements.\\n\n",
    "    Table part will start from '<table-start>' and ends with '<table-end>'\"\"\"\n",
    "    user_prompt = f\"\"\"Provide detailed and concise summary of the table or text that is well optimized for retrieval.\\n\\nTable or text: {element} \"\"\"\n",
    "    text_summary = _gpt_text_summary(client,sys_prompt,user_prompt)\n",
    "    return text_summary.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8d96e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_summary = []\n",
    "for content in data['content'].tolist():\n",
    "    text_summary.append(process_single_text_chunk(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85dd491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['summary'] = text_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0fb1ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('table_text.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "387541cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "import uuid\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be57fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_filename(filename):\n",
    "    # Define the regex pattern\n",
    "    import re\n",
    "    pattern = re.compile(r'image(\\d+)_\\d+\\.png')\n",
    "    \n",
    "    # Use the regex pattern to find a match in the filename\n",
    "    match = pattern.match(filename)\n",
    "    \n",
    "    # If there's a match, return the extracted number as an integer\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # If no match is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80152e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['page_number'] = dataframe['image_name'].apply(extract_number_from_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eccfa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_df, image_df\n",
    "):\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    def add_documents(retriever, doc_summaries, doc_contents,page_number):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i],'page_number':page_number[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    if isinstance(text_df, pd.DataFrame):\n",
    "        add_documents(retriever, text_df['summary'].tolist(), text_df['content'].tolist(),text_df['page_number'].tolist())\n",
    "    if isinstance(image_df, pd.DataFrame):\n",
    "        add_documents(retriever, image_df['summary'].tolist(), image_df['encoding'].tolist(),image_df['page_number'].tolist())\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8d26af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.SSLError: HTTPSConnectionPool(host='app.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 0.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='app.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 3.2s (requests.exceptions.SSLError: HTTPSConnectionPool(host='app.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.62 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='app.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=\"text-embedding-ada-002\",\n",
    "        openai_api_version=\"2023-05-15\",\n",
    "    )\n",
    "vectorstore = Chroma(\n",
    "    collection_name='<name of collection>', embedding_function=embed_model,persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = create_multi_vector_retriever(vectorstore,data,dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68cf5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "## store mapping of summary and actual chunk\n",
    "with open('chroma_db/<filename>.json', \"w\") as json_file:\n",
    "    json.dump(retriever.docstore.store, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed6efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
